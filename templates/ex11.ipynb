{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import time\n",
    "from matplotlib.patches import Ellipse\n",
    "import scipy.stats\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# ensure that rerunning the notebook will give fixed results\n",
    "np.random.seed(2701)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ellipse plot from ex3 with added mean\n",
    "\n",
    "def plot_gaussian(ax, mean, cov, color='red', size=3):\n",
    "    # draws ellipses representing different standard deviations (size is the number of ellipses)\n",
    "    # width and height are times by 2 since sqrt of the eigenval only measures half the distance\n",
    "    eig_w, eig_v = np.linalg.eig(cov)\n",
    "    for i in range(size):\n",
    "        ell = Ellipse(xy=[mean[0], mean[1]], \n",
    "                      width=np.sqrt(eig_w[0]) * 2 * (i + 1), \n",
    "                      height=np.sqrt(eig_w[1]) * 2 * (i + 1), \n",
    "                      angle=np.rad2deg(np.arccos(eig_v[0, 0])), \n",
    "                      edgecolor=color, lw=2, facecolor='none')\n",
    "        ax.add_artist(ell)\n",
    "    print(mean)\n",
    "    ax.plot(mean[0], mean[1], \"x\", c=color, ms=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\star$ Question 1\n",
    "\n",
    "Implement the Box-Muller method to produce $N$ samples from an isotropic Gaussian distribution of dimension $D$ with mean $\\mu \\in \\mathbb{R}^D$  and covariance $\\sigma^2 I \\in \\mathbb{R}^{D \\times D}$ given a uniform proposal distribution. $I$ is the identity matrix. See [wikipedia](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) for the math. Use [np.random.uniform](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html) for sampling the uniform distribution.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* In a multivariate isotropic Gaussian the dimensions are independent of each other and can therefore be sampled independently.\n",
    "* The Box-Muller method will give you a standard Gaussian distribution with mean 0 and standard deviation 1. Scale it by the standard deviation and add the mean to get an arbitrary Gaussian distribution.\n",
    "\n",
    "Plot some samples and visualize the standard deviation with the utility function `plot_gaussian` above.\n",
    "\n",
    "For comparison, plot samples with [scipy.stats.multivariate_normal.rvs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html) and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def box_muller(n_samples:int, dim:int, mean:np.ndarray, std:float):\n",
    "    # TODO\n",
    "    return samples\n",
    "\n",
    "# test function with 11 samples of a 3d gaussian \n",
    "gaussian_samples_3d = box_muller(11, 3, np.array([0., 0., 0.]), 1.)\n",
    "assert gaussian_samples_3d.shape == (11, 3)\n",
    "\n",
    "# define 2d gaussian\n",
    "mean_2d = np.array([10., 20.])\n",
    "std_2d = 5.\n",
    "n = 10000\n",
    "\n",
    "# TODO sample and plot the samples\n",
    "# TODO draw the standard deviation ellipsis in the same plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO sample and plot the same gaussian using scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\star$ $\\star$  Question 2\n",
    "\n",
    "Implement the Metropolis algorithm to sample from an anisotropic Gaussian distribution via an isotropic Gaussian proposal distribution. Use the box-muller method from above to sample from the proposal distribution.\n",
    "\n",
    "Mean $\\mu$ and covariance $\\Sigma$ of the anisotropic Gaussian are given below (note that the covariance is and must be nonsingular). You can create an anisotropic gaussian object `scipy.stats.multivariate_normal` and then use its function `pdf(x)` to get the probability density at point `x`. Note that here the probability $p(x)$ is given, however for the Metropolis algorithm an unnormalized function $f(x) = p(x) / Z$ with an unknown normalization constant $Z$ would work as well. This is one of the advantages of the algorithm as $Z$ can be hard or impossible to compute depending on the distribution.\n",
    "\n",
    "Choose the proposal distribution as $q(z|z^{(l)}) = \\mathcal{N}(z^{(l)}, \\sigma_q^2 I )$ and $q(z) = \\mathcal{N}(\\mu_q, \\sigma_q^2 I)$ and set the hyperparameters as $\\mu_q = \\mu$, $\\sigma_q = 0.1$.\n",
    "\n",
    "Produce 10000 samples and plot them and the standard deviation ellipses. Note that you will need more trials than the number of samples since some samples will be discarded. You can safeguard against division by zero to avoid warnings.\n",
    "\n",
    "Compare this with directly sampling from the anisotropic Gaussian with its method `rvs`. What do you observe about the quality of the approximation? Can you improve it? Also, the master solution takes about 0.5 second to produce 10000 samples. Can you match this performance or even create a faster implementation?\n",
    "\n",
    "Aside from comparing the samples of the distributions visually, estimate mean and covariance from the samples as in exercise 2 with `np.mean(samples, axis=0)` and `np.cov(samples.T)` and plot the standard deviation ellipses of your fit. You could then also evaluate the [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Kullback%E2%80%93Leibler_divergence) between the original gaussian and the gaussian estimated from the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([-1.26745097,  0.1375625 ])\n",
    "cov = np.array([[3.10419125, 1.61656817], [1.61656817, 1.2180084 ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
